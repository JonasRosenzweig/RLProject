{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\kata\\anaconda3\\lib\\site-packages (0.17.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\kata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\kata\\anaconda3\\lib\\site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\kata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in c:\\users\\kata\\anaconda3\\lib\\site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: future in c:\\users\\kata\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: keras in c:\\users\\kata\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\kata\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kata\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\kata\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\kata\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\kata\\appdata\\roaming\\python\\python38\\site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model\n",
    "# from wandb.keras import WandbCallback\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# Inside my model training code\n",
    "# import wandb\n",
    "# wandb.init(project=\"my_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.counter = 0\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.rewards_list = []\n",
    "\n",
    "        self.replay_memory_buffer = deque(maxlen=500_000)\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.num_action_space = self.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "        self.model = self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        model = Sequential()\n",
    "        # This is where we add the NN?\n",
    "        model.add(Dense(128, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(Dropout(0.2))\n",
    "        # tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "        model.add(Dense(64, activation=relu))\n",
    "        model.add(Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=self.lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Make random / exploratory choice if random number smaller than current epsilon.\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        # Otherwise make exploitatory action, where the future states are predicted and an appropriate action taken.\n",
    "        predicted_actions = self.model.predict(state)\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn_and_update_weights_by_reply(self):\n",
    "\n",
    "        # replay_memory_buffer size check\n",
    "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
    "            return\n",
    "\n",
    "        # Early Stopping\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        random_sample = self.get_random_sample_from_replay_mem()\n",
    "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    def get_attribues_from_sample(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done_list = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
    "\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "\n",
    "  \n",
    "\n",
    "    def update_counter(self):\n",
    "        self.counter += 1\n",
    "        step_size = 5\n",
    "        self.counter = self.counter % step_size\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Run 0\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 9,668\n",
      "Trainable params: 9,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ep' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c1ad9b3523b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average Reward\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDQNAgent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"saved_models_\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-c1ad9b3523b7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(agent, can_stop)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m           \u001b[0mep_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m           \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Episode: {}, total_reward: {:.2f}, last_step_reward: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m   \u001b[0mrun_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m   \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ep' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "run_rewards = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "lr = 0.0005\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "\n",
    "def train(agent, can_stop = True):\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "      print(\"Training Run {}\".format(epoch))\n",
    "      ep_rewards = []\n",
    "      num_episodes = 2000\n",
    "      agent = DQNAgent(env, lr, epsilon, epsilon_decay, gamma)\n",
    "      \n",
    "      for eppisode in range(num_episodes):\n",
    "          state = env.reset()\n",
    "          total_reward = 0\n",
    "          num_steps = 1000\n",
    "          state = np.reshape(state, [1, agent.num_observation_space])\n",
    "          for step in range(num_steps):\n",
    "              env.render()\n",
    "              # time.sleep(0.0003)\n",
    "              received_action = agent.get_action(state)\n",
    "              next_state, reward, done, info = env.step(received_action)\n",
    "              next_state = np.reshape(next_state, [1, agent.num_observation_space])\n",
    "              agent.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
    "              total_reward += reward\n",
    "              state = next_state\n",
    "              agent.update_counter()\n",
    "              agent.learn_and_update_weights_by_reply()  \n",
    "\n",
    "              if done: \n",
    "                break\n",
    "\n",
    "              if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "              last_rewards_mean = np.mean(agent.rewards_list[-100:])\n",
    "              if last_rewards_mean > 200 and can_stop:\n",
    "                print(\"DQN Training Complete...\")\n",
    "                break\n",
    "\n",
    "          ep_rewards.append(total_reward)\n",
    "          print(\"Episode: {}, total_reward: {:.2f}, last_step_reward: {:.3f}\".format(ep, total_reward, reward))\n",
    "  run_rewards.append(ep_rewards)\n",
    "  env.close()\n",
    "\n",
    "  for n, ep_rewards in enumerate(run_rewards):\n",
    "    x = range(len(ep_rewards))\n",
    "    cumsum = np.cumsum(ep_rewards)\n",
    "    avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "    plt.plot(x, avgs)\n",
    "  plt.title(\"Agent Performance\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Average Reward\")\n",
    "\n",
    "train(DQNAgent, True)\n",
    "save_dir = \"saved_models_\"\n",
    "model = DQNAgent(env, lr, epsilon, epsilon_decay, gamma)\n",
    "model.save(save_dir + \"trained_model_Phi.h5\")\n"
   ]
  }
 ]
}