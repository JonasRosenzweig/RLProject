{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start buy installing the gym module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.models import load_model\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we can interact with the environment in OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# create the CartPole-v0 gym environment\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In OpenAI environments, the input we give our agents is taken from the Observation space. In this case, Box(4,) refers to a continuous space. If we go to the CartPole-v1 documentation:(https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) we can see the observation space gives us 4 continuous observation (input) options, and 2 discrete action (output) options.\n",
    "\n",
    "Now let's see how we can make a basic agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    # create basicAgent class\n",
    "    def __init__(self,env):\n",
    "        # constructor\n",
    "        self.action_size = env.action_space.n\n",
    "        # number of available actions, in this case 2\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        # method for choosing action\n",
    "        action = random.choice(range(self.action_size))\n",
    "        # choose random action \n",
    "        return action "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a basic agent that returns a random action, lets run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RandomAgent(env)\n",
    "# create agent object\n",
    "state = env.reset() \n",
    "# reset environment state and define initial state\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    # loop for 100 iterations\n",
    "    action = agent.get_action(state)\n",
    "    # get action from agent passing in the state\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # returns the state after action (env object)\n",
    "    # env.render()\n",
    "    # time.sleep(0.03)\n",
    "    if not done:\n",
    "        total_reward += reward\n",
    "env.close()\n",
    "print(\"Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each environment has its own \"Solved\" requirements, and some remain unsolved, as can be seen on the Leaderboard: https://github.com/openai/gym/wiki/Leaderboard For the CartPole-v1 env, \"done\" returns True when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center, and \"solving\" is defined as getting average reward of 195.0 over 100 consecutive trials. A reward of +1 is provided for every timestep that the pole remains upright. Done also returns true when the reward is 200.\n",
    "\n",
    "Obviously, our random agent isn't performing that well. Let's try and improve on this by giving it a non-random policy and interacting with the observation space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicPolicyAgent():\n",
    "    def __init__(self,env):\n",
    "        self.action_size = env.action_space.n\n",
    "    def get_action(self, state):\n",
    "        pole_angle = state[2]\n",
    "        # index[2] of observation space\n",
    "        action = 0 if pole_angle < 0 else 1\n",
    "        # policy action, move left is angle < 0 else right\n",
    "        return action\n",
    "\n",
    "agent = BasicPolicyAgent(env)\n",
    "state = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for _ in range(100):\n",
    "    action = agent.get_action(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    # time.sleep(0.03)\n",
    "    if not done:\n",
    "        total_reward += reward\n",
    "env.close()\n",
    "print(\"Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with a very simple policy, the behavior is improved. Now we've seen how to interact with the environment and create agents.\n",
    "\n",
    "Let's look at a simple implementation of the HillClimbing method. Essentially, we want a function that takes all the inputs (observations) available and outputs a vector with the predicted values of the next state of the environment based on a given action.\n",
    "\n",
    "Essentially we want to transform our input state vector [1 4] to a [1 2] output action vector. We do this by multiplying the [1 4] matrix with a [4 2] weights matrix.\n",
    "\n",
    "This weights matrix is initialized with random weights. We use this initial weights matrix to get our action (output) vector from our observation (input) vector. We calculate the total reward from the episode, and save it as our best reward. We also save the weights matrix as our best weights. We then sample a new random weights matrix, to which we add a random noise matrix of the same dimensions for variance. If the new total reward is higher than the best total reward, we use the weights that gave us this new higher reward and save them as best weights, and halve the noise matrix to decrease variance. If the new total reward is lower, we keep the old weights and double the noise matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HillClimbingAgent():\n",
    "    def __init__(self, env):\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        # shape or dimension of the env's observation_shape matrix\n",
    "        self.action_size = env.action_space.n\n",
    "        self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "            self.weights = 1e-4*np.random.rand(*self.state_dim, self.action_size)\n",
    "            # initialize random weights matrix\n",
    "            self.best_reward = -np.Inf\n",
    "            # initialize reward as lowest possible number\n",
    "            self.best_weights = np.copy(self.weights)\n",
    "            # initialize best_weights as first random weights matrix\n",
    "            self.noise_scale = 1e-2\n",
    "            # initialize noise scale\n",
    "            \n",
    "        \n",
    "    def get_action(self, state):\n",
    "            p = np.dot(state, self.weights)\n",
    "            # dot product of state and weights\n",
    "            action = np.argmax(p)\n",
    "            # action is highest element in p\n",
    "            return action\n",
    "        \n",
    "    def update_model(self, reward):\n",
    "            if reward >= self.best_reward:\n",
    "                self.best_reward = reward\n",
    "                self.best_weights = np.copy(self.weights)\n",
    "                self.noise_scale = max(self.noise_scale/2, 1e-3)\n",
    "            else:\n",
    "                self.noise_scale = min(self.noise_scale*2, 2)\n",
    "                    \n",
    "            self.weights = self.best_weights + self.noise_scale * np.random.rand(*self.state_dim, self.action_size)\n",
    "            # sample new current weights by adding best weight and noise matrix scaled by new noise scale\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)\n",
    "num_runs = 1\n",
    "run_rewards = []\n",
    "\n",
    "for n in range(num_runs):\n",
    "  print(\"Run {}\".format(n))\n",
    "  ep_rewards = []\n",
    "  agent = HillClimbingAgent(env)\n",
    "  num_episodes = 200\n",
    "\n",
    "  for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = agent.get_action(state)\n",
    "      state, reward, done, info = env.step(action)\n",
    "      # env.render()\n",
    "      total_reward += reward\n",
    "      # time.sleep(0.03)\n",
    "\n",
    "    ep_rewards.append(total_reward)\n",
    "    agent.update_model(total_reward)\n",
    "    print(\"Episode: {}, total_reward: {:.2f}\".format(ep, total_reward))\n",
    "run_rewards.append(ep_rewards)\n",
    "env.close()\n",
    "\n",
    "for n, ep_rewards in enumerate(run_rewards):\n",
    "  x = range(len(ep_rewards))\n",
    "  cumsum = np.cumsum(ep_rewards)\n",
    "  avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "  plt.plot(x, avgs)\n",
    "plt.title(\"HillClimbingAgent Performance\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After just 200 episodes, we can see that the agent is now able to keep the pole upright for the maximum 200 frames consistently (depending on how lucky you are with the random initialized weights matrix, this may take more episodes).\n",
    "\n",
    "Let's try this method in a different gym environment. The acrobot system includes two joints and two links, where the joint between the two links is actuated. Initially, the links are hanging downwards, and the goal is to swing the end of the lower link up to a given height. The action is either applying +1, 0 or -1 torque on the joint between the two pendulum links. The Observation space or state consists of the sin() and cos() of the two rotational joint angles and the joint angular velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Acrobot-v1\"\n",
    "env = gym.make(env_name)\n",
    "num_runs = 1\n",
    "run_rewards = []\n",
    "\n",
    "for n in range(num_runs):\n",
    "  print(\"Run {}\".format(n))\n",
    "  ep_rewards = []\n",
    "  agent = HillClimbingAgent(env)\n",
    "  num_episodes = 200\n",
    "\n",
    "  for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = agent.get_action(state)\n",
    "      state, reward, done, info = env.step(action)\n",
    "      # env.render()\n",
    "      total_reward += reward\n",
    "      # time.sleep(0.03)\n",
    "\n",
    "    ep_rewards.append(total_reward)\n",
    "    agent.update_model(total_reward)\n",
    "    print(\"Episode: {}, total_reward: {:.2f}\".format(ep, total_reward))\n",
    "run_rewards.append(ep_rewards)\n",
    "env.close()\n",
    "\n",
    "for n, ep_rewards in enumerate(run_rewards):\n",
    "  x = range(len(ep_rewards))\n",
    "  cumsum = np.cumsum(ep_rewards)\n",
    "  avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "  plt.plot(x, avgs)\n",
    "plt.title(\"Agent Performance\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's introduce Q-Learning; it is a reinforcement learning algorithm that aims to find the best action to take given a current state to maximize reward. This algorithm is considered off-policy due to the fact the Q-learning function learns from actions outside the current policy, like taking random actions.\n",
    "\n",
    "The first step is to create a Q-table (or matrix) that follows the shape of [state, action] and we initialize each value in the matrix to zero. We update and store our Q-values in this matrix after each episode. This then becomes a reference table for our agent to select the best action based on the highest Q-value.\n",
    "\n",
    "The second step is to interact with the environment, and update the state-action pairs in our Q-table. This is done in one of two ways: exploiting, or exploring. When exploiting, the Q-table is used as reference to view all possible actions for a given state, and the agent selects the action based on the max value of said action. When exploring, the action is taken randomly. This is important as it allows the agent to explore and discover new states that may not be selected during exploitation. These two modes of interaction with the environment are balanced using a variable usually named epsilon, which determines the probability of exploring versus exploiting.\n",
    "\n",
    "We've mentioned epsilon (exploration probability), one of the hyperparameters, there are others that are useful. The learning rate, sometimes denoted alpha, is how much an updated value is \"accepted\" vs the old one. The difference between the new and old value is multiplied by this learning rate and added to the previous q-value. A discount factor, usually denoted gamma, can be used to balance immediate and future reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    def __init__(self, env, buckets=(3, 3, 6, 6,), min_alpha=0.1, min_epsilon=0.1, gamma=1.0, ada_divisor=20):\n",
    "        self.env = env # for choosing different environments\n",
    "        self.buckets = buckets # down-scaling feature space to discrete range\n",
    "        self.min_alpha = min_alpha # learning rate\n",
    "        self.min_epsilon = min_epsilon # exploration rate\n",
    "        self.gamma = gamma # discount factor\n",
    "        self.ada_divisor = ada_divisor # decay rate parameter for alpha and epsilon\n",
    "\n",
    "        # initialising Q-table\n",
    "        self.Q = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "    # Discretizing input space to make Q-table and to reduce dimmensionality\n",
    "    def discretize(self, state):\n",
    "        upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50)]\n",
    "        lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50)]\n",
    "        ratios = [(state[i] + abs(lower_bounds[i])) / (upper_bounds[i] - lower_bounds[i]) for i in range(len(state))]\n",
    "        discretized_state = [int(round((self.buckets[i] - 1) * ratios[i])) for i in range(len(state))]\n",
    "        discretized_state = [min(self.buckets[i] - 1, max(0, discretized_state[i])) for i in range(len(state))]\n",
    "        return tuple(discretized_state)\n",
    "\n",
    "    # Choosing action based on epsilon-greedy policy\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.Q[state])\n",
    "\n",
    "    # Updating Q-value of state-action pair based on the Bellman equation\n",
    "    def update_q(self, state, action, reward, next_state, alpha):\n",
    "        self.Q[state][action] += alpha * (reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state][action])\n",
    "\n",
    "    # Reduce Exploration Rate Over time\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.min_epsilon, min(1, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n",
    "\n",
    "    # Reduce Learning Rate over time\n",
    "    def get_alpha(self, t):\n",
    "        return max(self.min_alpha, min(1.0, 1.0 - math.log10((t + 1) / self.ada_divisor)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "run_rewards = []\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "for n in range(num_runs):\n",
    "  print(\"Run {}\".format(n))\n",
    "  ep_rewards = []\n",
    "  num_episodes = 200\n",
    "  agent = QAgent(env)\n",
    "\n",
    "  for ep in range(num_episodes):\n",
    "    # As states are continuous, discretize them into buckets\n",
    "    discretized_state = agent.discretize(env.reset())\n",
    "\n",
    "    # Get adaptive learning alpha and epsilon decayed over time\n",
    "    alpha = agent.get_alpha(ep)\n",
    "    epsilon = agent.get_epsilon(ep)\n",
    "            \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Choose action according to greedy policy and take it\n",
    "        action = agent.choose_action(discretized_state, epsilon)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        next_state = agent.discretize(state)\n",
    "        # Update Q-Table\n",
    "        agent.update_q(discretized_state, action, reward, next_state, alpha)\n",
    "        discretized_state = next_state\n",
    "        i += 1\n",
    "        # env.render()\n",
    "        total_reward += reward\n",
    "        # time.sleep(0.03)\n",
    "    ep_rewards.append(total_reward)\n",
    "    print(\"Episode: {}, total_reward: {:.2f}\".format(ep, total_reward))\n",
    "run_rewards.append(ep_rewards)\n",
    "env.close()\n",
    "\n",
    "for n, ep_rewards in enumerate(run_rewards):\n",
    "  x = range(len(ep_rewards))\n",
    "  cumsum = np.cumsum(ep_rewards)\n",
    "  avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "  plt.plot(x, avgs)\n",
    "plt.title(\"Agent Performance\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.counter = 0\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.rewards_list = []\n",
    "\n",
    "        self.replay_memory_buffer = deque(maxlen=500000)\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.num_action_space = self.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "        self.model = self.initialize_model()\n",
    "\n",
    "    def initialize_model(self):\n",
    "        model = Sequential()\n",
    "        # This is where we add the NN?\n",
    "        model.add(Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(Dense(256, activation=relu))\n",
    "        model.add(Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=mean_squared_error,optimizer=Adam(lr=self.lr))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Make random / exploratory choice if random number smaller than current epsilon.\n",
    "            return random.randrange(self.num_action_space)\n",
    "\n",
    "        # Otherwise make exploitatory action, where the future states are predicted and an appropriate action taken.\n",
    "        predicted_actions = self.model.predict(state)\n",
    "        return np.argmax(predicted_actions[0])\n",
    "\n",
    "    def add_to_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn_and_update_weights_by_reply(self):\n",
    "\n",
    "        # replay_memory_buffer size check\n",
    "        if len(self.replay_memory_buffer) < self.batch_size or self.counter != 0:\n",
    "            return\n",
    "\n",
    "        # Early Stopping\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        random_sample = self.get_random_sample_from_replay_mem()\n",
    "        states, actions, rewards, next_states, done_list = self.get_attribues_from_sample(random_sample)\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(next_states), axis=1)) * (1 - done_list)\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    def get_attribues_from_sample(self, random_sample):\n",
    "        states = np.array([i[0] for i in random_sample])\n",
    "        actions = np.array([i[1] for i in random_sample])\n",
    "        rewards = np.array([i[2] for i in random_sample])\n",
    "        next_states = np.array([i[3] for i in random_sample])\n",
    "        done_list = np.array([i[4] for i in random_sample])\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "        return np.squeeze(states), actions, rewards, next_states, done_list\n",
    "\n",
    "    def get_random_sample_from_replay_mem(self):\n",
    "        random_sample = random.sample(self.replay_memory_buffer, self.batch_size)\n",
    "        return random_sample\n",
    "\n",
    "  \n",
    "\n",
    "    def update_counter(self):\n",
    "        self.counter += 1\n",
    "        step_size = 5\n",
    "        self.counter = self.counter % step_size\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "run_rewards = []\n",
    "env = gym.make('LunarLander-v2')\n",
    "lr = 0.001\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "def train(agent, can_stop = True):\n",
    "  for n in range(num_runs):\n",
    "      print(\"Training Run {}\".format(n))\n",
    "      ep_rewards = []\n",
    "      num_episodes = 3\n",
    "      agent = DQNAgent(env, lr, epsilon, epsilon_decay, gamma)\n",
    "      \n",
    "      for ep in range(num_episodes):\n",
    "          state = env.reset()\n",
    "          total_reward = 0\n",
    "          num_steps = 1000\n",
    "          state = np.reshape(state, [1, agent.num_observation_space])\n",
    "          for step in range(num_steps):\n",
    "              # env.render()\n",
    "              # time.sleep(0.0003)\n",
    "              received_action = agent.get_action(state)\n",
    "              next_state, reward, done, info = env.step(received_action)\n",
    "              next_state = np.reshape(next_state, [1, agent.num_observation_space])\n",
    "              agent.add_to_replay_memory(state, received_action, reward, next_state, done)\n",
    "              total_reward += reward\n",
    "              state = next_state\n",
    "              agent.update_counter()\n",
    "              agent.learn_and_update_weights_by_reply()  \n",
    "\n",
    "              if done: \n",
    "                break\n",
    "\n",
    "              if agent.epsilon > agent.epsilon_min:\n",
    "                agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "              last_rewards_mean = np.mean(agent.rewards_list[-100:])\n",
    "              if last_rewards_mean > 200 and can_stop:\n",
    "                print(\"DQN Training Complete...\")\n",
    "                break\n",
    "\n",
    "          ep_rewards.append(total_reward)\n",
    "          print(\"Episode: {}, total_reward: {:.2f}, last_step_reward: {:.3f}\".format(ep, total_reward, reward))\n",
    "  run_rewards.append(ep_rewards)\n",
    "  env.close()\n",
    "\n",
    "  for n, ep_rewards in enumerate(run_rewards):\n",
    "    x = range(len(ep_rewards))\n",
    "    cumsum = np.cumsum(ep_rewards)\n",
    "    avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "    plt.plot(x, avgs)\n",
    "  plt.title(\"Agent Performance\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Average Reward\")\n",
    "\n",
    "train(DQNAgent, True)\n",
    "save_dir = \"saved_models_\"\n",
    "model = DQNAgent(env, lr, epsilon, epsilon_decay, gamma)\n",
    "model.save(save_dir + \"trained_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "run_rewards = []\n",
    "\n",
    "def test_trained_model(agent, trained_model):\n",
    "    for n in range(num_runs):\n",
    "      print(\"Testing Run {}\".format(n))\n",
    "      ep_rewards = []\n",
    "      num_episodes = 10\n",
    "      agent = DQNAgent(env, lr, epsilon, epsilon_decay, gamma)\n",
    "\n",
    "      for ep in range(num_episodes):\n",
    "          current_state = env.reset()\n",
    "          num_observation_space = env.observation_space.shape[0]\n",
    "          current_state = np.reshape(current_state, [1, num_observation_space])\n",
    "          total_reward = 0\n",
    "          num_steps = 1000\n",
    "\n",
    "          for step in range(num_steps):\n",
    "              # env.render()\n",
    "              # time.sleep(0.0003)\n",
    "              selected_action = np.argmax(trained_model.predict(current_state)[0])\n",
    "              new_state, reward, done, info = env.step(selected_action)\n",
    "              new_state = np.reshape(new_state, [1, num_observation_space])\n",
    "              current_state = new_state\n",
    "              total_reward += reward\n",
    "              if done:\n",
    "                  break\n",
    "          ep_rewards.append(total_reward)\n",
    "          print(\"Episode: {}, total_reward: {:.2f}, last_step_reward: {:.3f}\".format(ep, total_reward, reward))\n",
    "    run_rewards.append(ep_rewards)\n",
    "    env.close()\n",
    "\n",
    "    for n, ep_rewards in enumerate(run_rewards):\n",
    "        x = range(len(ep_rewards))\n",
    "        cumsum = np.cumsum(ep_rewards)\n",
    "        avgs = [cumsum[ep]/(ep+1) if ep<100 else (cumsum[ep]-cumsum[ep-100])/100 for ep in x]\n",
    "        plt.plot(x, avgs)\n",
    "    plt.title(\"Agent Performance\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_models_\"\n",
    "trained_model = load_model(save_dir + \"trained_model.h5\")\n",
    "test_trained_model(DQNAgent, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3209603dc16fa08139850b0fbcec5529b2035f6d8448c78a588ebbadffc6ff1e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}